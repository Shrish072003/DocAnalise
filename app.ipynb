{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6a6bfe8c1eca487686e49fd0fd9cf6bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c9e67b84fc2442a59c13cf8f511e0433",
              "IPY_MODEL_0db7279fe3a54783a5432bb41ae312e1",
              "IPY_MODEL_1fbe60cdc0dd432dab39217937bf59a5"
            ],
            "layout": "IPY_MODEL_5536b7bcd4e046e7a30c567cc089a469"
          }
        },
        "c9e67b84fc2442a59c13cf8f511e0433": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef67bb5f8a3c4cf68691701f2434c96d",
            "placeholder": "​",
            "style": "IPY_MODEL_1d01a0989b7c4f49a0ba35225dbda6f4",
            "value": "Downloading (…)chat.ggmlv3.q5_1.bin: 100%"
          }
        },
        "0db7279fe3a54783a5432bb41ae312e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9b239ffa78a47548b2f8e4754575d0c",
            "max": 9763701888,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_de5e827177e34426b702c39789a22a94",
            "value": 9763701888
          }
        },
        "1fbe60cdc0dd432dab39217937bf59a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddd38b0241734b3097b2dd39df322a59",
            "placeholder": "​",
            "style": "IPY_MODEL_70d997b392c7434f9b0df721734abdb0",
            "value": " 9.76G/9.76G [01:27&lt;00:00, 150MB/s]"
          }
        },
        "5536b7bcd4e046e7a30c567cc089a469": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef67bb5f8a3c4cf68691701f2434c96d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d01a0989b7c4f49a0ba35225dbda6f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9b239ffa78a47548b2f8e4754575d0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de5e827177e34426b702c39789a22a94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ddd38b0241734b3097b2dd39df322a59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70d997b392c7434f9b0df721734abdb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers langchain PyPDF2 langchain\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRD9zX9llHrS",
        "outputId": "aa0748df-2ab0-4509-e5f5-4e487b62228d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence_transformers\n",
            "  Using cached sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting langchain\n",
            "  Using cached langchain-0.0.266-py3-none-any.whl (1.5 MB)\n",
            "Collecting PyPDF2\n",
            "  Using cached pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence_transformers)\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence_transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence_transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.19)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.21 (from langchain)\n",
            "  Downloading langsmith-0.0.22-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.5)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic<2,>=1 (from langchain)\n",
            "  Downloading pydantic-1.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (9.4.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125924 sha256=36f3366623738ee7f3c8522b0115ee90e480a75879448e87bcaee0ccf0c26e3f\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, PyPDF2, pydantic, mypy-extensions, marshmallow, typing-inspect, openapi-schema-pydantic, langsmith, huggingface-hub, transformers, dataclasses-json, langchain, sentence_transformers\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.1.1\n",
            "    Uninstalling pydantic-2.1.1:\n",
            "      Successfully uninstalled pydantic-2.1.1\n",
            "Successfully installed PyPDF2-3.0.1 dataclasses-json-0.5.14 huggingface-hub-0.16.4 langchain-0.0.266 langsmith-0.0.22 marshmallow-3.20.1 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 pydantic-1.10.12 safetensors-0.3.2 sentence_transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.31.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install InstructorEmbedding\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7cXdrvxqjEN",
        "outputId": "0cd4384c-fc0d-4c8d-d6b2-31a0c280a435"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: InstructorEmbedding in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-gpu faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Puo_y_Rj2GAL",
        "outputId": "7f65fb11-4b6a-48c7-cda6-257ea88de2eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.10/dist-packages (1.7.2)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install install llama-cpp-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sy0GoyDrStmI",
        "outputId": "0a37376d-424d-42fa-f440-d2fffc99b351"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: install in /usr/local/lib/python3.10/dist-packages (1.3.5)\n",
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.10/dist-packages (0.1.77)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.7.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9s1HJI7wYgs4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00569a74-abc9-4898-bc9b-4c4e69bd349c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-c0dbeb5d9165>:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import trange\n"
          ]
        }
      ],
      "source": [
        "import concurrent.futures\n",
        "# from .instructor import *\n",
        "import torch\n",
        "from tqdm.autonotebook import trange\n",
        "from PyPDF2 import PdfReader\n",
        "import InstructorEmbedding\n",
        "from transformers import T5Tokenizer\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "# from InstructorEmbedding import INSTRUCTOR\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def get_pdf_text(pdf_docs):\n",
        "    text = \"\"\n",
        "    for pdf in pdf_docs:\n",
        "        pdf_reader = PdfReader(pdf)\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "def get_text_chunks(text, chunk_size=256):\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        separator=\"\\n\",\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=100,\n",
        "        length_function=len\n",
        "    )\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks\n",
        "\n",
        "# def get_vectorstore_parallel(text_chunks, executor):\n",
        "#     # Initialize HuggingFaceInstructEmbeddings\n",
        "#     embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\")\n",
        "\n",
        "#     # Initialize Hugging Face model and tokenizer\n",
        "#     model_name = \"hkunlp/instructor-xl\"\n",
        "#     tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "#     # Define the process_batch function\n",
        "#     def process_batch(batch):\n",
        "#         inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "#         with torch.no_grad():\n",
        "#             outputs = embeddings.encode(batch)  # Use the embeddings to encode the batch\n",
        "#         return outputs\n",
        "\n",
        "#     batch_size = 4\n",
        "#     encoded_chunks = []\n",
        "#     for i in range(0, len(text_chunks), batch_size):\n",
        "#         batch = text_chunks[i:i + batch_size]\n",
        "#         results = list(executor.map(process_batch, batch))\n",
        "#         for result in results:\n",
        "#             encoded_chunks.extend(result)\n",
        "\n",
        "#     vectorstore = FAISS.from_embeddings(embeddings=encoded_chunks, batch_size=batch)\n",
        "#     return vectorstore\n",
        "\n",
        "def get_vectorstore(text_chunks):\n",
        "    # embeddings = OpenAIEmbeddings()\n",
        "    embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\")\n",
        "    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n",
        "    return vectorstore\n",
        "\n",
        "# def get_vectorstore_parallel(text_chunks, executor):\n",
        "#     embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\")\n",
        "\n",
        "#     # Define the function to process a chunk\n",
        "#     def process_chunk(chunk):\n",
        "#         # Assuming there's a different method to obtain embeddings from the embeddings object\n",
        "#         # Replace this line with the appropriate method\n",
        "#         embeddings_result = embeddings.get_embeddings(chunk)\n",
        "#         return embeddings_result\n",
        "\n",
        "#     batch_size = 4\n",
        "#     encoded_chunks = []\n",
        "#     for i in range(0, len(text_chunks), batch_size):\n",
        "#         batch = text_chunks[i:i + batch_size]\n",
        "#         results = list(executor.map(process_chunk, batch))\n",
        "#         encoded_chunks.extend(results)\n",
        "\n",
        "#     # Assuming there's a different way to create a vector store from embeddings\n",
        "#     # Replace this line with the appropriate method\n",
        "#     vectorstore = create_vectorstore_from_embeddings(encoded_chunks)\n",
        "\n",
        "#     return vectorstore\n",
        "\n",
        "\n",
        "\n",
        "def get_conversation_chain(vectorstore):\n",
        "    # model_path=\"llama-2-13b-chat.ggmlv3.q5_1.bin\" # https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML\n",
        "    llm_model_path = \"/content/llama-2-13b-chat.ggmlv3.q5_1.bin\"  # Replace with your LLM model file path\n",
        "\n",
        "    n_gpu_layers = 40\n",
        "    # n_gpu_layers = 20\n",
        "\n",
        "    n_batch = 256\n",
        "    # n_batch = 156\n",
        "\n",
        "    callback_handler = StreamingStdOutCallbackHandler()  # Initialize the specific callback handler\n",
        "    callback_manager = CallbackManager(handlers=[callback_handler])\n",
        "\n",
        "    # Loading model,\n",
        "    llm = LlamaCpp(\n",
        "        model_path=llm_model_path,\n",
        "        max_tokens=256,\n",
        "        n_gpu_layers=n_gpu_layers,\n",
        "        n_batch=n_batch,\n",
        "        callback_manager=callback_manager,\n",
        "        n_ctx=2024,\n",
        "        verbose=False,\n",
        "    )\n",
        "\n",
        "    memory = ConversationBufferMemory(\n",
        "        memory_key='chat_history', return_messages=True)\n",
        "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=vectorstore.as_retriever(),\n",
        "        memory=memory\n",
        "    )\n",
        "    return conversation_chain\n",
        "\n",
        "def handle_userinput(user_question, conversation_chain):\n",
        "    # Replace st.session_state with an appropriate data structure to hold the chat history\n",
        "    chat_history = []\n",
        "\n",
        "    response = conversation_chain({'question': user_question})\n",
        "    chat_history.extend(response['chat_history'])\n",
        "\n",
        "    for i, message in enumerate(chat_history):\n",
        "        if i % 2 == 0:\n",
        "            print(message.content)\n",
        "        else:\n",
        "            print(message.content)\n",
        "\n"
      ],
      "metadata": {
        "id": "qfdVH4A1Zqwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_file_path = \"/content/RDE_progresses.pdf\"\n",
        "\n",
        "raw_text = get_pdf_text([pdf_file_path])  # Pass the PDF file path as a list\n",
        "\n",
        "# Rest of your code\n"
      ],
      "metadata": {
        "id": "G0BqzA8ieB_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunks = get_text_chunks(raw_text)"
      ],
      "metadata": {
        "id": "gayYITI-k1ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text_chunks"
      ],
      "metadata": {
        "id": "fxmDD3LVk3It"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text_chunks = [chunk.to(device) for chunk in text_chunks]\n",
        "\n",
        "# text_chunks = []\n",
        "\n",
        "# num_workers = 4\n",
        "\n",
        "# with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "#     # vectorstore = get_vectorstore_parallel(text_chunks, executor)\n",
        "#     text_chunks_gpu = [chunk.to(device) for chunk in text_chunks]\n",
        "#     vectorstore = get_vectorstore(text_chunks)\n",
        "    # vectorstore = vectorstore.to(device)\n",
        "\n",
        "\n",
        "\n",
        "vectorstore = get_vectorstore(get_text_chunks)\n",
        "conversation_chain = get_conversation_chain(vectorstore)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "ZFZ7FXVCk-Zx",
        "outputId": "d174c6c4-ed5b-4271-9933-972e8b25e2fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-3bd851b4238f>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mvectorstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vectorstore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_text_chunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mconversation_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_conversation_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorstore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-db5244382222>\u001b[0m in \u001b[0;36mget_vectorstore\u001b[0;34m(text_chunks)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# embeddings = OpenAIEmbeddings()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHuggingFaceInstructEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hkunlp/instructor-xl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mvectorstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFAISS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvectorstore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/vectorstores/faiss.py\u001b[0m in \u001b[0;36mfrom_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    605\u001b[0m                 \u001b[0mfaiss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFAISS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         \"\"\"\n\u001b[0;32m--> 607\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m         return cls.__from(\n\u001b[1;32m    609\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/embeddings/huggingface.py\u001b[0m in \u001b[0;36membed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \"\"\"\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0minstruction_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_instruction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstruction_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'function' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Example usage\n",
        "# # pdf_files = [\"file1.pdf\", \"file2.pdf\"]  # Replace with your PDF filenames\n",
        "# # pdf_files = [\"/content/file1.pdf\", \"/content/file2.pdf\"]  # Replace with your PDF filenames\n",
        "# pdf_files =\"/RDE_progresses.pdf\"\n",
        "\n",
        "# raw_text = get_pdf_text(pdf_files)\n",
        "# text_chunks = get_text_chunks(raw_text)\n",
        "\n",
        "# num_workers = 4\n",
        "# with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "#     vectorstore = get_vectorstore_parallel(text_chunks, executor)\n",
        "\n",
        "# conversation_chain = get_conversation_chain(vectorstore)\n",
        "\n",
        "# # Example user input\n",
        "# user_question = \"What is this document about?\"\n",
        "# handle_userinput(user_question, conversation_chain)\n"
      ],
      "metadata": {
        "id": "YqwmlOLqhJoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "Omopg0pYhRWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "zKOLkZjvz2Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def get_pdf_text(pdf_docs):\n",
        "    text = \"\"\n",
        "    for pdf in pdf_docs:\n",
        "        pdf_reader = PdfReader(pdf)\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "def get_text_chunks(text, chunk_size=256):\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        separator=\"\\n\",\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=100,\n",
        "        length_function=len\n",
        "    )\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def get_vectorstore_parallel(text_chunks, executor):\n",
        "    embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\")\n",
        "\n",
        "    # Define the function to process a chunk\n",
        "    def process_chunk(chunk):\n",
        "        # Assuming there's a different method to obtain embeddings from the embeddings object\n",
        "        # Replace this line with the appropriate method\n",
        "        embeddings_result = embeddings.get_embeddings(chunk)\n",
        "        return embeddings_result\n",
        "\n",
        "    batch_size = 4\n",
        "    encoded_chunks = []\n",
        "    for i in range(0, len(text_chunks), batch_size):\n",
        "        batch = text_chunks[i:i + batch_size]\n",
        "        results = list(executor.map(process_chunk, batch))\n",
        "        encoded_chunks.extend(results)\n",
        "\n",
        "    vectorstore = create_vectorstore_from_embeddings(encoded_chunks)\n",
        "\n",
        "    return vectorstore\n",
        "\n",
        "\n",
        "\n",
        "def get_conversation_chain(vectorstore):\n",
        "    # model_path=\"llama-2-13b-chat.ggmlv3.q5_1.bin\" # https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML\n",
        "    llm_model_path = \"/content/llama-2-13b-chat.ggmlv3.q5_1.bin\"  # Replace with your LLM model file path\n",
        "\n",
        "    n_gpu_layers = 40\n",
        "    # n_gpu_layers = 20\n",
        "\n",
        "    n_batch = 256\n",
        "    # n_batch = 156\n",
        "\n",
        "    callback_handler = StreamingStdOutCallbackHandler()  # Initialize the specific callback handler\n",
        "    callback_manager = CallbackManager(handlers=[callback_handler])\n",
        "\n",
        "    # Loading model,\n",
        "    llm = LlamaCpp(\n",
        "        model_path=llm_model_path,\n",
        "        max_tokens=256,\n",
        "        n_gpu_layers=n_gpu_layers,\n",
        "        n_batch=n_batch,\n",
        "        callback_manager=callback_manager,\n",
        "        n_ctx=2024,\n",
        "        verbose=False,\n",
        "    )\n",
        "\n",
        "    memory = ConversationBufferMemory(\n",
        "        memory_key='chat_history', return_messages=True)\n",
        "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=vectorstore.as_retriever(),\n",
        "        memory=memory\n",
        "    )\n",
        "    return conversation_chain\n",
        "\n",
        "def handle_userinput(user_question, conversation_chain):\n",
        "    # Replace st.session_state with an appropriate data structure to hold the chat history\n",
        "    chat_history = []\n",
        "\n",
        "    response = conversation_chain({'question': user_question})\n",
        "    chat_history.extend(response['chat_history'])\n",
        "\n",
        "    for i, message in enumerate(chat_history):\n",
        "        if i % 2 == 0:\n",
        "            print(message.content)\n",
        "        else:\n",
        "            print(message.content)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XM6K5DPj0sSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunks = [chunk.to(device) for chunk in text_chunks]\n"
      ],
      "metadata": {
        "id": "MoJzCNM36c2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunks = []\n",
        "\n",
        "num_workers = 4\n",
        "\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "    vectorstore = get_vectorstore_parallel(text_chunks, executor)\n",
        "    # text_chunks_gpu = [chunk.to(device) for chunk in text_chunks]\n",
        "    vectorstore = get_vectorstore(text_chunks)\n",
        "    # vectorstore = vectorstore.to(device)"
      ],
      "metadata": {
        "id": "egInghjT6eit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "conversation_chain = get_conversation_chain(vectorstore)\n",
        "\n",
        "# Example user input\n",
        "user_question = \"What is this document about?\"\n",
        "handle_userinput(user_question, conversation_chain)"
      ],
      "metadata": {
        "id": "dkL5I9FY6q88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M9-jp-wU7FqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JO3M7c9x7HMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "567BIYjt7HVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8FQ4c1WV7HYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rxNRW-TP7Hcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uaMtNbdZ7Hfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent.futures\n",
        "import torch\n",
        "from transformers import T5Tokenizer, BertModel, BertTokenizer\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# Load the GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def get_pdf_text(pdf_docs):\n",
        "    text = \"\"\n",
        "    for pdf in pdf_docs:\n",
        "        pdf_reader = PdfReader(pdf)\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "def get_text_chunks(text, chunk_size=256):\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        separator=\"\\n\",\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=100,\n",
        "        length_function=len\n",
        "    )\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# def get_vectorstore_parallel(text_chunks, executor):\n",
        "#     # Initialize HuggingFaceInstructEmbeddings\n",
        "#     embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\")\n",
        "\n",
        "#     batch_size = 4\n",
        "#     encoded_chunks = []\n",
        "#     for i in range(0, len(text_chunks), batch_size):\n",
        "#         batch = text_chunks[i:i + batch_size]\n",
        "#         results = list(executor.map(embeddings.encode, batch))\n",
        "#         for result in results:\n",
        "#             encoded_chunks.extend(result)\n",
        "\n",
        "#     vectorstore = FAISS.from_embeddings(embeddings=encoded_chunks, batch_size=batch)\n",
        "#     return vectorstore\n",
        "\n",
        "\n",
        "\n",
        "# def get_vectorstore_parallel(text_chunks, executor):\n",
        "#     # Initialize HuggingFace tokenizer and model\n",
        "#     model_name = \"bert-base-uncased\"\n",
        "#     tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "#     model = BertModel.from_pretrained(model_name)\n",
        "#     model.to(device)\n",
        "\n",
        "#     batch_size = 4\n",
        "#     encoded_chunks = []\n",
        "#     for i in range(0, len(text_chunks), batch_size):\n",
        "#         batch = text_chunks[i:i + batch_size]\n",
        "#         inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "#         with torch.no_grad():\n",
        "#             inputs.to(device)\n",
        "#             outputs = model(**inputs)\n",
        "#             embedding = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()\n",
        "#         encoded_chunks.extend(embedding)\n",
        "\n",
        "#     vectorstore = FAISS.from_embeddings(embeddings=encoded_chunks, batch_size=batch)\n",
        "#     return vectorstore\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def get_vectorstore_parallel(text_chunks, executor):\n",
        "#     # Initialize HuggingFace tokenizer and model\n",
        "#     model_name = \"bert-base-uncased\"\n",
        "#     tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "#     model = BertModel.from_pretrained(model_name)\n",
        "#     model.to(device)\n",
        "\n",
        "#     batch_size = 4\n",
        "#     text_embeddings = []  # Collect individual text embeddings\n",
        "#     for i in range(0, len(text_chunks), batch_size):\n",
        "#         batch = text_chunks[i:i + batch_size]\n",
        "#         inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "#         with torch.no_grad():\n",
        "#             inputs.to(device)\n",
        "#             outputs = model(**inputs)\n",
        "#             embedding = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()\n",
        "#             text_embeddings.extend(embedding)\n",
        "\n",
        "#     # Create FAISS vector store from embeddings\n",
        "#     vectorstore = FAISS.from_embeddings(text_embeddings, batch_size=batch)\n",
        "#     return vectorstore\n",
        "\n",
        "\n",
        "\n",
        "# def get_vectorstore_parallel(text_chunks, executor):\n",
        "#     # Initialize HuggingFace tokenizer and model\n",
        "#     model_name = \"bert-base-uncased\"\n",
        "#     tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "#     model = BertModel.from_pretrained(model_name)\n",
        "#     model.to(device)\n",
        "\n",
        "#     batch_size = 4\n",
        "#     text_embeddings = []  # Collect individual text embeddings\n",
        "#     for i in range(0, len(text_chunks), batch_size):\n",
        "#         batch = text_chunks[i:i + batch_size]\n",
        "#         inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "#         with torch.no_grad():\n",
        "#             inputs.to(device)\n",
        "#             outputs = model(**inputs)\n",
        "#             embedding = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()\n",
        "#             text_embeddings.append(embedding)\n",
        "\n",
        "#     # Concatenate the embeddings to create a single tensor\n",
        "#     text_embeddings = torch.tensor(text_embeddings).to(device)\n",
        "\n",
        "#     # Create FAISS vector store from embeddings\n",
        "#     vectorstore = FAISS.from_embeddings(embeddings=text_embeddings)\n",
        "#     return vectorstore\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def get_vectorstore_parallel(text_chunks, executor):\n",
        "#     # Initialize HuggingFace tokenizer and model\n",
        "#     model_name = \"bert-base-uncased\"\n",
        "#     tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "#     model = BertModel.from_pretrained(model_name)\n",
        "#     model.to(device)\n",
        "\n",
        "#     batch_size = 4\n",
        "#     text_embeddings = []  # Collect individual text embeddings\n",
        "#     for i in range(0, len(text_chunks), batch_size):\n",
        "#         batch = text_chunks[i:i + batch_size]\n",
        "#         inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "#         with torch.no_grad():\n",
        "#             inputs.to(device)\n",
        "#             outputs = model(**inputs)\n",
        "#             embedding = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()\n",
        "#             text_embeddings.extend(embedding)  # Extend the list of embeddings\n",
        "\n",
        "#     # Convert the list of embeddings to a tensor\n",
        "#     text_embeddings = torch.tensor(text_embeddings).to(device)\n",
        "\n",
        "#     # Create FAISS vector store from embeddings\n",
        "#     vectorstore = FAISS.from_embeddings(embeddings=text_embeddings)\n",
        "#     return vectorstore\n",
        "\n",
        "\n",
        "\n",
        "# def get_vectorstore_parallel(text_chunks, executor):\n",
        "#     # Initialize HuggingFace tokenizer and model\n",
        "#     model_name = \"bert-base-uncased\"\n",
        "#     tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "#     model = BertModel.from_pretrained(model_name)\n",
        "#     model.to(device)\n",
        "\n",
        "#     batch_size = 4\n",
        "#     text_embeddings = []  # Collect individual text embeddings\n",
        "#     for i in range(0, len(text_chunks), batch_size):\n",
        "#         batch = text_chunks[i:i + batch_size]\n",
        "#         inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "#         with torch.no_grad():\n",
        "#             inputs.to(device)\n",
        "#             outputs = model(**inputs)\n",
        "#             embedding = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()\n",
        "#             text_embeddings.extend(embedding)  # Extend the list of embeddings\n",
        "\n",
        "#     # Convert the list of embeddings to a tensor\n",
        "#     text_embeddings = torch.tensor(text_embeddings).to(device)\n",
        "\n",
        "#     # Convert embeddings to NumPy array for FAISS\n",
        "#     text_embeddings_np = text_embeddings.cpu().numpy()\n",
        "\n",
        "#     # Create FAISS vector store from embeddings\n",
        "#     vectorstore = FAISS.from_embeddings(text_embeddings_np)\n",
        "#     return vectorstore\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_vectorstore_parallel(text_chunks, executor):\n",
        "    # Initialize HuggingFace tokenizer and model\n",
        "    model_name = \"bert-base-uncased\"\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    model = BertModel.from_pretrained(model_name)\n",
        "    model.to(device)\n",
        "\n",
        "    batch_size = 4\n",
        "    text_embeddings = []  # Collect individual text embeddings\n",
        "    for i in range(0, len(text_chunks), batch_size):\n",
        "        batch = text_chunks[i:i + batch_size]\n",
        "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "        with torch.no_grad():\n",
        "            inputs.to(device)\n",
        "            outputs = model(**inputs)\n",
        "            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()\n",
        "            text_embeddings.extend(embedding)  # Extend the list of embeddings\n",
        "\n",
        "    # Convert the list of embeddings to a tensor\n",
        "    # text_embeddings = torch.tensor(text_embeddings).to(device)\n",
        "\n",
        "    # Convert embeddings to NumPy array for FAISS\n",
        "    text_embeddings_np = text_embeddings.numpy()\n",
        "\n",
        "    # Create FAISS vector store from embeddings\n",
        "    vectorstore = FAISS.from_embeddings(embeddings=text_embeddings_np, batch_size=batch_size)\n",
        "    return vectorstore\n",
        "\n",
        "\n",
        "\n",
        "# def get_vectorstore_parallel(text_chunks, executor):\n",
        "#     # Initialize HuggingFace tokenizer and model\n",
        "#     model_name = \"bert-base-uncased\"\n",
        "#     tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "#     model = BertModel.from_pretrained(model_name)\n",
        "#     model.to(device)\n",
        "\n",
        "#     batch_size = 4\n",
        "#     text_embeddings = []  # Collect individual text embeddings\n",
        "#     for i in range(0, len(text_chunks), batch_size):\n",
        "#         batch = text_chunks[i:i + batch_size]\n",
        "#         inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "#         with torch.no_grad():\n",
        "#             inputs.to(device)\n",
        "#             outputs = model(**inputs)\n",
        "#             embedding = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()\n",
        "#             text_embeddings.extend(embedding)\n",
        "\n",
        "#     # Create FAISS vector store from embeddings\n",
        "#     vectorstore = FAISS.from_embeddings(text_embeddings)\n",
        "#     return vectorstore\n",
        "\n",
        "\n",
        "\n",
        "def get_conversation_chain(vectorstore):\n",
        "    llm_model_path = \"/content/llama-2-13b-chat.ggmlv3.q5_1.bin\"  # Replace with your LLM model file path\n",
        "    n_gpu_layers = 40\n",
        "    n_batch = 256\n",
        "\n",
        "    callback_handler = StreamingStdOutCallbackHandler()\n",
        "    callback_manager = CallbackManager(handlers=[callback_handler])\n",
        "\n",
        "    llm = LlamaCpp(\n",
        "        model_path=llm_model_path,\n",
        "        max_tokens=256,\n",
        "        n_gpu_layers=n_gpu_layers,\n",
        "        n_batch=n_batch,\n",
        "        callback_manager=callback_manager,\n",
        "        n_ctx=2024,\n",
        "        verbose=False,\n",
        "    )\n",
        "\n",
        "    memory = ConversationBufferMemory(\n",
        "        memory_key='chat_history', return_messages=True)\n",
        "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=vectorstore.as_retriever(),\n",
        "        memory=memory\n",
        "    )\n",
        "    return conversation_chain\n",
        "\n",
        "# Load PDF files and extract text\n",
        "pdf_files =[\"/content/RDE_progresses.pdf\"]  # Replace with your PDF filenames\n",
        "raw_text = get_pdf_text(pdf_files)\n",
        "text_chunks = get_text_chunks(raw_text)\n",
        "\n",
        "# Number of workers for parallel processing\n",
        "num_workers = 4\n",
        "\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "    # text_chunks_gpu = [chunk.to(device) for chunk in text_chunks]\n",
        "    # vectorstore = get_vectorstore_parallel(text_chunks_gpu, executor)\n",
        "    vectorstore = get_vectorstore_parallel(text_chunks, executor)\n",
        "\n",
        "    vectorstore = vectorstore.to(device)\n",
        "\n",
        "conversation_chain = get_conversation_chain(vectorstore)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "-vAKQPdC7HiR",
        "outputId": "6ff505fc-f7c7-4f0a-959a-04d60d5452de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-5c1b18cbab67>\u001b[0m in \u001b[0;36m<cell line: 277>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;31m# text_chunks_gpu = [chunk.to(device) for chunk in text_chunks]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;31m# vectorstore = get_vectorstore_parallel(text_chunks_gpu, executor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m     \u001b[0mvectorstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vectorstore_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0mvectorstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-5c1b18cbab67>\u001b[0m in \u001b[0;36mget_vectorstore_parallel\u001b[0;34m(text_chunks, executor)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;31m# Convert embeddings to NumPy array for FAISS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0mtext_embeddings_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;31m# Create FAISS vector store from embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'numpy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MC8LDPn_3RG",
        "outputId": "f1fc3082-de52-4d3a-bcc4-a433baea4822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n"
      ],
      "metadata": {
        "id": "prYlM_v0Dllr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
        "model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6a6bfe8c1eca487686e49fd0fd9cf6bc",
            "c9e67b84fc2442a59c13cf8f511e0433",
            "0db7279fe3a54783a5432bb41ae312e1",
            "1fbe60cdc0dd432dab39217937bf59a5",
            "5536b7bcd4e046e7a30c567cc089a469",
            "ef67bb5f8a3c4cf68691701f2434c96d",
            "1d01a0989b7c4f49a0ba35225dbda6f4",
            "a9b239ffa78a47548b2f8e4754575d0c",
            "de5e827177e34426b702c39789a22a94",
            "ddd38b0241734b3097b2dd39df322a59",
            "70d997b392c7434f9b0df721734abdb0"
          ]
        },
        "id": "NOqPw0WxEyeL",
        "outputId": "37f520a1-0421-4913-ca6b-310947e5c6ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)chat.ggmlv3.q5_1.bin:   0%|          | 0.00/9.76G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a6bfe8c1eca487686e49fd0fd9cf6bc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# simple\n",
        "\n",
        "import concurrent.futures\n",
        "# import streamlit as st\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from transformers import AutoModel\n",
        "from langchain.llms import HuggingFaceHub\n",
        "\n",
        "# from htmlTemplates import css, bot_template, user_template\n",
        "\n",
        "\n",
        "\n",
        "def get_pdf_text(pdf_docs):\n",
        "    text = \"\"\n",
        "    for pdf in pdf_docs:\n",
        "        pdf_reader = PdfReader(pdf)\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "\n",
        "def get_text_chunks(text, chunk_size=1000):\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        separator=\"\\n\",\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=100,\n",
        "        length_function=len\n",
        "    )\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def get_vectorstore(text_chunks):\n",
        "    embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\")\n",
        "    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n",
        "    return vectorstore\n",
        "\n",
        "\n",
        "def get_conversation_chain(vectorstore):\n",
        "    # model_path=\"llama-2-13b-chat.ggmlv3.q5_1.bin\" # https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML\n",
        "    # model_name = \"TheBloke/Llama-2-13B-chat-GGML\"  # Hugging Face model name\n",
        "    # model = HuggingFaceHub(repo_id=\"google/flan-t5-xxl\")\n",
        "    # from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "    # tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xxl\")\n",
        "    # model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-xxl\")\n",
        "\n",
        "\n",
        "    model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
        "    model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format\n",
        "\n",
        "    from huggingface_hub import hf_hub_download\n",
        "\n",
        "    model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
        "\n",
        "    n_gpu_layers = 40\n",
        "    # n_gpu_layers = 20\n",
        "\n",
        "    n_batch = 256\n",
        "    # n_batch = 156\n",
        "\n",
        "    callback_handler = StreamingStdOutCallbackHandler()  # Initialize the specific callback handler\n",
        "    callback_manager = CallbackManager(handlers=[callback_handler])\n",
        "\n",
        "    # Loading model,\n",
        "    llm = LlamaCpp(\n",
        "        model_path=model_path,\n",
        "        max_tokens=256,\n",
        "        temperature=0.5,\n",
        "        n_gpu_layers=n_gpu_layers,\n",
        "        n_batch=n_batch,\n",
        "        callback_manager=callback_manager,\n",
        "        n_ctx=2024,\n",
        "        verbose=False,\n",
        "    )\n",
        "\n",
        "    memory = ConversationBufferMemory(\n",
        "        memory_key='chat_history', return_messages=True)\n",
        "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=vectorstore.as_retriever(),\n",
        "        memory=memory\n",
        "    )\n",
        "    return conversation_chain\n",
        "\n",
        "\n",
        "\n",
        "def handle_userinput(user_question, conversation_chain):\n",
        "    # Replace st.session_state with an appropriate data structure to hold the chat history\n",
        "    chat_history = []\n",
        "\n",
        "    response = conversation_chain({'question': user_question})\n",
        "    chat_history.extend(response['chat_history'])\n",
        "\n",
        "    for i, message in enumerate(chat_history):\n",
        "        if i % 2 == 0:\n",
        "            print(message.content)\n",
        "        else:\n",
        "            print(message.content)\n",
        "\n",
        "\n",
        "# Load PDF files and extract text\n",
        "pdf_files =[\"/content/RDE_progresses.pdf\"]  # Replace with your PDF filenames\n",
        "raw_text = get_pdf_text(pdf_files)\n",
        "text_chunks = get_text_chunks(raw_text)\n",
        "\n",
        "# create vector store\n",
        "vectorstore = get_vectorstore(text_chunks)\n",
        "conversation_chain = get_conversation_chain(vectorstore)\n",
        "\n",
        "# Example user input\n",
        "user_question = \"What is this document about?\"\n",
        "handle_userinput(user_question, conversation_chain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgcQQr3r7IdI",
        "outputId": "18a4e7b2-3498-4786-bc86-7830370c0ede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n",
            " This document discusses pulse detonation engines (PDEs) and their research from 2007 to 2014, including experimental work and novel concepts for space applications. It also compares different engine cycles and discusses the challenges of par"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " This document discusses pulse detonation engines (PDEs) and their research from 2007 to 2014, including experimental work and novel concepts for space applications. It also compares different engine cycles and discusses the challenges of par...\n"
      ],
      "metadata": {
        "id": "CRnHa04MxQyb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **fine tune the** embiddings, store, llm\n",
        "\n"
      ],
      "metadata": {
        "id": "-SKeACHBRmjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit langchain PyPDF2 sentence_transformers InstructorEmbedding faiss-gpu faiss-cpu llama-cpp-python transformers"
      ],
      "metadata": {
        "id": "zLb0AmTbVGHQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7308f4e3-770b-4eb6-fa07-c4291e0ce6d6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.25.0-py2.py3-none-any.whl (8.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain\n",
            "  Downloading langchain-0.0.266-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting InstructorEmbedding\n",
            "  Downloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl (19 kB)\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.1.77.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.1)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.6)\n",
            "Requirement already satisfied: importlib-metadata<7,>=1.4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.8.0)\n",
            "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.23.5)\n",
            "Requirement already satisfied: packaging<24,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit) (23.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.5.3)\n",
            "Requirement already satisfied: pillow<10,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=6.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Collecting pympler<2,>=0.9 (from streamlit)\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.18 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.5.2)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.2.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.7.1)\n",
            "Collecting tzlocal<5,>=1.1 (from streamlit)\n",
            "  Downloading tzlocal-4.3.1-py3-none-any.whl (20 kB)\n",
            "Collecting validators<1,>=0.2 (from streamlit)\n",
            "  Downloading validators-0.21.2-py3-none-any.whl (25 kB)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8 (from streamlit)\n",
            "  Downloading pydeck-0.8.0-py2.py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.1)\n",
            "Collecting watchdog>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.19)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.21 (from langchain)\n",
            "  Downloading langsmith-0.0.22-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.5)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic<2,>=1 (from langchain)\n",
            "  Downloading pydantic-1.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.15.2+cu118)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence_transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence_transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7,>=1.4->streamlit) (3.16.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.16.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.18->streamlit) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.18->streamlit) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.18->streamlit) (2023.7.22)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.6)\n",
            "Collecting pytz-deprecation-shim (from tzlocal<5,>=1.1->streamlit)\n",
            "  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.9.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting tzdata (from pytz-deprecation-shim->tzlocal<5,>=1.1->streamlit)\n",
            "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence_transformers, llama-cpp-python\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125924 sha256=43f28f9a66ad40c7d936dc28e1c2d542e0221d1975fb569e3e04498c2aec7288\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.77-cp310-cp310-linux_x86_64.whl size=276100 sha256=5b64306ace0425fa96e041f0fc4fca12e1cdf3c1bc0e8cb28b66fbef99518aac\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/ed/39/87f2ad350dbbf13b600ac744899186b8647c5323c62e2bb348\n",
            "Successfully built sentence_transformers llama-cpp-python\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, InstructorEmbedding, faiss-gpu, faiss-cpu, watchdog, validators, tzdata, smmap, PyPDF2, pympler, pydantic, mypy-extensions, marshmallow, diskcache, typing-inspect, pytz-deprecation-shim, pydeck, openapi-schema-pydantic, llama-cpp-python, langsmith, huggingface-hub, gitdb, tzlocal, transformers, gitpython, dataclasses-json, langchain, streamlit, sentence_transformers\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.1.1\n",
            "    Uninstalling pydantic-2.1.1:\n",
            "      Successfully uninstalled pydantic-2.1.1\n",
            "  Attempting uninstall: tzlocal\n",
            "    Found existing installation: tzlocal 5.0.1\n",
            "    Uninstalling tzlocal-5.0.1:\n",
            "      Successfully uninstalled tzlocal-5.0.1\n",
            "Successfully installed InstructorEmbedding-1.0.1 PyPDF2-3.0.1 dataclasses-json-0.5.14 diskcache-5.6.1 faiss-cpu-1.7.4 faiss-gpu-1.7.2 gitdb-4.0.10 gitpython-3.1.32 huggingface-hub-0.16.4 langchain-0.0.266 langsmith-0.0.22 llama-cpp-python-0.1.77 marshmallow-3.20.1 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 pydantic-1.10.12 pydeck-0.8.0 pympler-1.0.1 pytz-deprecation-shim-0.1.0.post0 safetensors-0.3.2 sentence_transformers-2.2.2 sentencepiece-0.1.99 smmap-5.0.0 streamlit-1.25.0 tokenizers-0.13.3 transformers-4.31.0 typing-inspect-0.9.0 tzdata-2023.3 tzlocal-4.3.1 validators-0.21.2 watchdog-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import concurrent.futures\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from transformers import AutoModel\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "\n",
        "def get_pdf_text(pdf_docs):\n",
        "    text = \"\"\n",
        "    for pdf in pdf_docs:\n",
        "        pdf_reader = PdfReader(pdf)\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "\n",
        "def get_text_chunks(text, chunk_size=1000):\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        separator=\"\\n\",\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=100,\n",
        "        length_function=len\n",
        "    )\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def get_vectorstore(text_chunks):\n",
        "    embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\")\n",
        "    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n",
        "    return vectorstore\n",
        "\n",
        "\n",
        "def get_conversation_chain(vectorstore):\n",
        "\n",
        "    model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
        "    model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format\n",
        "\n",
        "    model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
        "\n",
        "    n_gpu_layers = 40\n",
        "    n_batch = 256\n",
        "\n",
        "    callback_handler = StreamingStdOutCallbackHandler()  # Initialize the specific callback handler\n",
        "    callback_manager = CallbackManager(handlers=[callback_handler])\n",
        "\n",
        "    # Loading model,\n",
        "    llm = LlamaCpp(\n",
        "        model_path=model_path,\n",
        "        max_tokens=256,\n",
        "        temperature=0.5,\n",
        "        n_gpu_layers=n_gpu_layers,\n",
        "        n_batch=n_batch,\n",
        "        callback_manager=callback_manager,\n",
        "        n_ctx=2024,\n",
        "        verbose=False,\n",
        "    )\n",
        "\n",
        "    memory = ConversationBufferMemory(\n",
        "        memory_key='chat_history', return_messages=True)\n",
        "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=vectorstore.as_retriever(),\n",
        "        memory=memory\n",
        "    )\n",
        "    return conversation_chain\n",
        "\n",
        "\n",
        "\n",
        "def handle_userinput(user_question, conversation_chain):\n",
        "    # Replace st.session_state with an appropriate data structure to hold the chat history\n",
        "    chat_history = []\n",
        "\n",
        "    response = conversation_chain({'question': user_question})\n",
        "    chat_history.extend(response['chat_history'])\n",
        "\n",
        "    for i, message in enumerate(chat_history):\n",
        "        if i % 2 == 0:\n",
        "            print(message.content)\n",
        "        else:\n",
        "            print(message.content)\n",
        "\n",
        "\n",
        "\n",
        "# Load PDF files and extract text\n",
        "pdf_files =[\"/content/RDE_progresses.pdf\"]  # Replace with your PDF filenames\n",
        "raw_text = get_pdf_text(pdf_files)\n",
        "text_chunks = get_text_chunks(raw_text)\n",
        "\n",
        "# create vector store\n",
        "vectorstore = get_vectorstore(text_chunks)\n",
        "conversation_chain = get_conversation_chain(vectorstore)\n",
        "\n",
        "# Example user input\n",
        "user_question = \"What is this document about?\"\n",
        "handle_userinput(user_question, conversation_chain)"
      ],
      "metadata": {
        "id": "TuA1vaiWRlGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**streamlit appy exp**"
      ],
      "metadata": {
        "id": "3EmnuHImN5wt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit langchain PyPDF2 sentence_transformers InstructorEmbedding faiss-gpu faiss-cpu llama-cpp-python transformers"
      ],
      "metadata": {
        "id": "tRz1CQMkOWBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from /content/htmlTemplates.py import css, bot_template, user_template\n",
        "import concurrent.futures\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from transformers import AutoModel\n",
        "from langchain.llms import HuggingFaceHub\n",
        "\n",
        "def get_pdf_text(pdf_docs):\n",
        "    text = \"\"\n",
        "    for pdf in pdf_docs:\n",
        "        pdf_reader = PdfReader(pdf)\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "\n",
        "def get_text_chunks(text):\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        separator=\"\\n\",\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "        length_function=len\n",
        "    )\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def get_vectorstore(text_chunks):\n",
        "    embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\")\n",
        "    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n",
        "    return vectorstore\n",
        "\n",
        "\n",
        "def get_conversation_chain(vectorstore):\n",
        "    # llm = HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\":0.5, \"max_length\":512})\n",
        "    # model_path=\"llama-2-13b-chat.ggmlv3.q5_1.bin\" # https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML\n",
        "    # model_name = \"TheBloke/Llama-2-13B-chat-GGML\"  # Hugging Face model name\n",
        "\n",
        "    model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
        "    model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format\n",
        "\n",
        "    model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
        "\n",
        "\n",
        "    n_gpu_layers = 40\n",
        "    # n_gpu_layers = 20\n",
        "\n",
        "    n_batch = 256\n",
        "    # n_batch = 156\n",
        "\n",
        "    callback_handler = StreamingStdOutCallbackHandler()  # Initialize the specific callback handler\n",
        "    callback_manager = CallbackManager(handlers=[callback_handler])\n",
        "\n",
        "    # Loading model,\n",
        "    llm = LlamaCpp(\n",
        "        model_path=model_path,\n",
        "        max_tokens=256,\n",
        "        temperature=0.5,\n",
        "        n_gpu_layers=n_gpu_layers,\n",
        "        n_batch=n_batch,\n",
        "        callback_manager=callback_manager,\n",
        "        n_ctx=2024,\n",
        "        verbose=False,\n",
        "    )\n",
        "\n",
        "    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=model_path,\n",
        "        retriever=vectorstore.as_retriever(),\n",
        "        memory=memory\n",
        "    )\n",
        "    return conversation_chain\n",
        "\n",
        "\n",
        "def handle_userinput(user_question):\n",
        "    response = st.session_state.conversation({'question': user_question})\n",
        "    st.session_state.chat_history = response['chat_history']\n",
        "\n",
        "    for i, message in enumerate(st.session_state.chat_history):\n",
        "        if i % 2 == 0:\n",
        "            st.write(user_template.replace(\n",
        "                \"{{MSG}}\", message.content), unsafe_allow_html=True)\n",
        "        else:\n",
        "            st.write(bot_template.replace(\n",
        "                \"{{MSG}}\", message.content), unsafe_allow_html=True)\n",
        "\n",
        "\n",
        "def main():\n",
        "    # load_dotenv()\n",
        "    st.set_page_config(page_title=\"Chat with multiple PDFs\",\n",
        "                       page_icon=\":books:\")\n",
        "    st.write(css, unsafe_allow_html=True)\n",
        "\n",
        "    if \"conversation\" not in st.session_state:\n",
        "        st.session_state.conversation = None\n",
        "    if \"chat_history\" not in st.session_state:\n",
        "        st.session_state.chat_history = None\n",
        "\n",
        "    st.header(\"Chat with multiple PDFs :books:\")\n",
        "    user_question = st.text_input(\"Ask a question about your documents:\")\n",
        "    if user_question:\n",
        "        handle_userinput(user_question)\n",
        "\n",
        "    with st.sidebar:\n",
        "        st.subheader(\"Your documents\")\n",
        "        pdf_docs = st.file_uploader(\n",
        "            \"Upload your PDFs here and click on 'Process'\", accept_multiple_files=True)\n",
        "        if st.button(\"Process\"):\n",
        "            with st.spinner(\"Processing\"):\n",
        "                # get pdf text\n",
        "                raw_text = get_pdf_text(pdf_docs)\n",
        "\n",
        "                # get the text chunks\n",
        "                text_chunks = get_text_chunks(raw_text)\n",
        "\n",
        "                # create vector store\n",
        "                vectorstore = get_vectorstore(text_chunks)\n",
        "\n",
        "                # create conversation chain\n",
        "                st.session_state.conversation = get_conversation_chain(\n",
        "                    vectorstore)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "8g2gIMWJQ_0N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}